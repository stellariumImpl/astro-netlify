---
title: "Machine Learning & Data Mining"
description: "The revision of COMP9417"
image: "../assets/wallhaven-1p7853.png"
createdAt: 04-27-2025
draft: false
tags:
  - 模型分类
---

## Model Classification Methods

### Discriminative vs Generative

- 判别模型：学习条件概率P(Y|X)，也可以学习决策边界，目标是直接分类，不考虑数据是如何生成

    代表有逻辑回归、SVM、感知机perception、k近邻算法KNN、决策树、随机森林、梯度提升树（GBDT、XGBoost、LightGBM）Boosting框架下的判别模型、神经网络（MLP、CNN、RNN）非线性函数近似器。

- 生成模型：学习联合概率P(X,Y)，也可以学习边际概率P(X)或条件概率P(Y|X)，利用贝叶斯定理，然后用MAP进行分类。

    代表有朴素贝叶斯（假设特征条件独立估计P(X,Y)）、GMM（高斯混合模型）。

### Probabilistic vs Non-Probabilistic

- 概率模型：建模的是数据的概率分布，通常学习P(X)或P(X|Y)或联合分布P(X,Y)，模型的输出是一个概率值或者概率驱动的。

    代表有朴素贝叶斯分类器、贝叶斯网络、GMM（高斯混合模型）、逻辑回归。

- 非概率模型：不直接建立概率分布，而是输出一个确定性的函数结果（决策边界、标签值）。

    代表有决策树、SVM、k近邻算法KNN、感知机perception。

### Supervised vs Unsupervised

- 监督学习：训练数据包含输入特征和对应的标签，模型通过学习输入特征与标签之间的关系来进行预测。

    代表有线性回归、逻辑回归、决策树、随机森林、SVM、k近邻算法KNN、神经网络。

- 监督学习：训练数据包含输入特征和对应的标签，模型通过学习输入特征与标签之间的关系来进行预测。

    代表有线性回归、逻辑回归、决策树、随机森林、SVM、k近邻算法KNN、神经网络。

- 无监督学习：训练数据仅包含输入特征，没有标签，模型通过学习输入特征之间的关系来进行聚类或降维，目标是数据挖掘。

    代表有k均值聚类、层次聚类、主成分分析PCA、GMM（高斯混合模型）、Autoencoder（自编码器）。

### Linear vs Non-Linear

- 线性模型：模型输出是输入特征的线性组合。

    代表有线性回归、逻辑回归、感知机perception、线性SVM。

- 非线性模型：模型中包含非线性关系，不能用单一的线性函数来表示。

    代表有决策树、神经网络、非线性SVM（非线性核函数）、k近邻算法KNN。

### Stable vs Unstable

稳定性：用来描述学习算法对训练数据微小变化的敏感程度。

- 稳定模型：对于训练数据的小幅度变化不敏感，即使训练集略有改动，模型输出基本不变。

    代表有k近邻算法KNN和线性回归，他们的方差较低，预测结果平稳，所以通过集成学习方法（如Bagging）对其性能提升不明显。

- 不稳定模型：对于训练数据的小幅度变化敏感，即使训练集略有改动，模型输出也会发生较大变化。

    代表有决策树，他们具有较高的方差，容易过拟合训练数据，通过集成多个模型（如Bagging）可以有效降低方差，提升模型泛化能力。

## Model Errors and Generalization

### Generalization Error

训练误差与测试误差之间的差异，衡量模型从训练数据推广到新数据的能力。

- 可约误差

    - 偏差：指模型预测的期望与真实值之间的偏离，衡量模型预测能力，偏差高表示模型过于简单，不能很好地拟合数据。

    - 方差：指不同数据集下模型预测结果的波动性，衡量模型的稳定性，方差高表示模型对数据扰动敏感，容易过拟合。

- 不可约误差

    - 由系统固有的不确定性或噪声造成

    - 无法通过任何建模手段降低

### Overfitting vs Underfitting

- 过拟合：模型在训练集上表现很好，但在测试集上误差很大，原因是模型复杂度太高，高方差

    解决策略：

        - 简化模型

        - 增加数据量

        - 正则化：岭回归、Lasso回归、Dropout

- 欠拟合：模型在训练集和测试集上表现都不好，无法捕捉数据的基本结构，原因是模型太简单，偏差太高

    解决策略：

        - 增加模型复杂度

        - 使用更强的模型（加隐藏层，非线性激活）

        - 减少正则化力度

## Regularization

正则化是防止过拟合的一种方法，通过在损失函数中添加一个惩罚项来限制模型的复杂度。

下面公式中，第一项是标准的MSE，用于度量预测值和真实值的误差，第二项是正则项，控制参数大小，用于抑制过拟合。

正则化强度$\lambda$过大，模型变得过于简单，可能会欠拟合；反之，则会接近普通的最小二乘法OLS，可能会过拟合。

- 岭回归（L2）：

    $$
        J(\theta) = \sum_{j=1}^{m} (y_j - h_\theta(x_j))^2 + \lambda \sum_{i} \theta_i^2
    $$

 
- Lasso回归（L1）：

    $$
        J(\theta) = \sum_{j=1}^{m} (y_j - h_\theta(x_j))^2 + \lambda \sum_{i} |\theta_i|
    $$

    可以将某些参数收缩到零，实现特征选择，适用于高维数据，能够自动筛选重要变量，得到稀疏解。