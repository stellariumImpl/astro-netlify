---
title: "Machine Learning & Data Mining"
description: "The revision of COMP9417"
image: "../assets/wallhaven-1p7853.png"
createdAt: 04-30-2025
draft: false
tags:
  - 模型分类
  - 模型误差
  - 正则化
  - MLE
  - MAP
  - 最小二乘法与最大似然估计
---

## Model Classification Methods

### Discriminative vs Generative

- 判别模型：学习条件概率P(Y|X)，也可以学习决策边界，目标是直接分类，不考虑数据是如何生成

    代表有逻辑回归、SVM、感知机perception、k近邻算法KNN、决策树、随机森林、梯度提升树（GBDT、XGBoost、LightGBM）Boosting框架下的判别模型、神经网络（MLP、CNN、RNN）非线性函数近似器。

- 生成模型：学习联合概率P(X,Y)，也可以学习边际概率P(X)或条件概率P(Y|X)，利用贝叶斯定理，然后用MAP（最大后验概率估计）进行分类。

    代表有朴素贝叶斯（假设特征条件独立估计P(X,Y)）、GMM（高斯混合模型）。

### Probabilistic vs Non-Probabilistic

- 概率模型：建模的是数据的概率分布，通常学习P(X)或P(X|Y)或联合分布P(X,Y)，模型的输出是一个概率值或者概率驱动的。

    代表有朴素贝叶斯分类器、贝叶斯网络、GMM（高斯混合模型）、逻辑回归。

- 非概率模型：不直接建立概率分布，而是输出一个确定性的函数结果（决策边界、标签值）。

    代表有决策树、SVM、k近邻算法KNN、感知机perception。

### Supervised vs Unsupervised

- 监督学习：训练数据包含输入特征和对应的标签，模型通过学习输入特征与标签之间的关系来进行预测。

    代表有线性回归、逻辑回归、决策树、随机森林、SVM、k近邻算法KNN、神经网络。

- 监督学习：训练数据包含输入特征和对应的标签，模型通过学习输入特征与标签之间的关系来进行预测。

    代表有线性回归、逻辑回归、决策树、随机森林、SVM、k近邻算法KNN、神经网络。

- 无监督学习：训练数据仅包含输入特征，没有标签，模型通过学习输入特征之间的关系来进行聚类或降维，目标是数据挖掘。

    代表有k均值聚类、层次聚类、主成分分析PCA、GMM（高斯混合模型）、Autoencoder（自编码器）。

### Linear vs Non-Linear

- 线性模型：模型输出是输入特征的线性组合。

    代表有线性回归、逻辑回归、感知机perception、线性SVM。

- 非线性模型：模型中包含非线性关系，不能用单一的线性函数来表示。

    代表有决策树、神经网络、非线性SVM（非线性核函数）、k近邻算法KNN。

### Stable vs Unstable

稳定性：用来描述学习算法对训练数据微小变化的敏感程度。

- 稳定模型：对于训练数据的小幅度变化不敏感，即使训练集略有改动，模型输出基本不变。

    代表有k近邻算法KNN和线性回归，他们的方差较低，预测结果平稳，所以通过集成学习方法（如Bagging）对其性能提升不明显。

- 不稳定模型：对于训练数据的小幅度变化敏感，即使训练集略有改动，模型输出也会发生较大变化。

    代表有决策树，他们具有较高的方差，容易过拟合训练数据，通过集成多个模型（如Bagging）可以有效降低方差，提升模型泛化能力。

## Model Errors and Generalization

### Generalization Error

训练误差与测试误差之间的差异，衡量模型从训练数据推广到新数据的能力。

- 可约误差

    - 偏差：指模型预测的期望与真实值之间的偏离，衡量模型预测能力，偏差高表示模型过于简单，不能很好地拟合数据。

    - 方差：指不同数据集下模型预测结果的波动性，衡量模型的稳定性，方差高表示模型对数据扰动敏感，容易过拟合。

- 不可约误差

    - 由系统固有的不确定性或噪声造成

    - 无法通过任何建模手段降低

### Overfitting vs Underfitting

- 过拟合：模型在训练集上表现很好，但在测试集上误差很大，原因是模型复杂度太高，高方差

    解决策略：

        - 简化模型

        - 增加数据量

        - 正则化：岭回归、Lasso回归、Dropout

- 欠拟合：模型在训练集和测试集上表现都不好，无法捕捉数据的基本结构，原因是模型太简单，偏差太高

    解决策略：

        - 增加模型复杂度

        - 使用更强的模型（加隐藏层，非线性激活）

        - 减少正则化力度

## Regularization

正则化是防止过拟合的一种方法，通过在损失函数中添加一个惩罚项来限制模型的复杂度。

下面公式中，第一项是标准的MSE，用于度量预测值和真实值的误差，第二项是正则项，控制参数大小，用于抑制过拟合。

正则化强度$\lambda$过大，模型变得过于简单，可能会欠拟合；反之，则会接近普通的最小二乘法OLS，可能会过拟合。

- 岭回归（L2）：

    $$
        J(\theta) = \sum_{j=1}^{m} (y_j - h_\theta(x_j))^2 + \lambda \sum_{i} \theta_i^2
    $$

 
- Lasso回归（L1）：

    $$
        J(\theta) = \sum_{j=1}^{m} (y_j - h_\theta(x_j))^2 + \lambda \sum_{i} |\theta_i|
    $$

    可以将某些参数收缩到零，实现特征选择，适用于高维数据，能够自动筛选重要变量，得到稀疏解。

## MLE vs MAP

两者都是常见的参数估计方法，主要区别是是否考虑了先验信息。

- 最大似然估计 MLE：是机器学习中最广泛的训练策略，通过最大化训练数据在模型下出现的概率，来估计模型参数

    假设所有假设的先验概率是相等的，所以只看数据的似然

    $$
        \hat{\theta}_{MLE} = \arg\max_\theta P(X|\theta)
    $$

    所以可以说MLE是在没有先验知识的情况下“凭数据说话”的估计方法

    应用：许多经典模型（逻辑回归，GMM）都使用了MLE，特别的，朴素贝叶斯虽然看起来用了先验，但它是假设条件独立后再用MLE拟合条件概率

    **OLS（最小二乘法）与MLE的关系**

    OLS是在高斯噪声的前提下，最大似然估计的一种形式，回归目标变成了“在高斯分布下使观测数据最可能”的参数估计

    可以这样理解，如果假设误差服从正态分布，最小二乘法其实就是在做MLE

- 最大后验估计 MAP：在MLE的基础上，加入了先验知识，通过最大化后验概率来估计模型参数

    $$
        \hat{\theta}_{MAP} = \arg\max_\theta P(\theta|X) = \arg\max_\theta P(X|\theta)P(\theta)
    $$

    其中$P(\theta)$是先验分布，$P(X|\theta)$是似然函数，MAP就是要找到使得后验概率最大的参数$\theta$

**两种方法的对比：**

有些模型，如贝叶斯网络、贝叶斯逻辑回归会显式引入先验知识并用MAP进行优化，L1、L2正则化也可以视为MAP的一种表现，他们隐含使用了先验分布约束模型参数

所以MAP本质上可以看作是加入正则化的MLE，在数据稀缺或者不确定性高的情况下，MAP更稳定

**注意：**

- Ridge回归使用了高斯先验，Lasso回归使用了拉普拉斯先验

- 感知机和SVM等判别式方法不使用概率建模，因此不基于MLE或者MAP

## Naive Bayes and Optimal Bayes

- 朴素贝叶斯：

    核心假设：在类别y给定的条件下，特征之间是条件独立的

    推理过程：
    
        - 学习：从训练数据中估计P(Y)：先验概率，从训练数据中估计$P(X_i|Y)$即每个特征在类别下的条件概率

        - 预测：对于每个可能的类别Y，计算$P(Y|X) \propto P(Y)\prod_i P(X_i|Y)$，选择概率最大的类别作为预测结果

    例题：

        | Gender | Label |
        |--------|-------|
        | Male   | Yes   |
        | Female | No    |
        | Male   | Yes   |
        | Female | No    |
        | Female | Yes   |


        我们要预测：新样本Gender=Female，它的类别是Yes or No

    优点：

        - 学习和预测速度快，计算带价低

        - 对高维特征空间较为稳定

        - 即使特征独立性不成立，也常常效果不错

    局限：

        - 强独立性假设在实际中往往不成立

        - 对特征之间存在强依赖的数据效果较差

        - 连续特征需要建模为特定分布（如高斯分布）

    对于缺失值：朴素贝叶斯天然适用于缺失值时，因为它假设特征条件独立，所以可以跳过缺失的特征，仅用已知的特征计算联合概率

    判别准则：

        - 似然比：等价于假设先验概率相等，即MLE

            $$
            likelihood Ratio = \frac{P(X|Y=1)}{P(X|Y=0)}
            $$

            关注某一个特征组合在不同列别下的相对可能性

        - 后验赔率：结合了先验概率和观测数据，更接近MAP原则

            $$
            posterior Odds = \frac{P(Y=1|X)}{P(Y=0|X)} = \frac{P(X|Y=1)P(Y=1)}{P(X|Y=0)P(Y=0)}
            $$

    零频率问题：当某个类别在训练集中没有出现时，直接计算条件概率会导致零概率，影响分类结果

        解决方法：

        - 拉普拉斯平滑：在每个类别的计数上加1，避免零概率

            $$
            P(X_i|Y) = \frac{count(X_i, Y) + 1}{count(Y) + |X|}
            $$

            其中$|X|$为特征总数

- 贝叶斯最优分类器：

    在给定特征X的情况下，选择后验概率最大的类别Y作为预测结果

    $$
        Y^* = \arg\max_Y P(Y|X) = \arg\max_Y P(X|Y)P(Y)
    $$

    其中$P(Y)$是先验概率，$P(X|Y)$是似然函数

    贝叶斯最优分类器是理论上最优的分类器，在所有可能的分类器中具有最低的错误率

    朴素贝叶斯是贝叶斯最优分类器的一种近似实现，假设特征条件独立

### Bayesian Expected Loss

如果不同类别的错误分类代价不同，则不能简单地使用最大后验概率则为决策，此时应该用最小化期望损失（Excepted Loss）方法

$$
\mathbb{E}[\text{Loss} \mid \hat{y}] = \sum_{y} L(y, \hat{y}) \cdot P(y \mid x)
$$

举个例子：

|实际/预测|肿瘤|正常|
|---|---|---|
|肿瘤|0|100|
|正常|1|0|

也就是说如果病人真实是肿瘤（Y=肿瘤），但你预测是正常（$\hat{Y}=$正常），代表损失100

如果真实是正常（Y=正常），你预测是肿瘤（$\hat{Y}=$肿瘤），损失1

然后我们还没知道病人的真实状况，是肿瘤还是正常，但我们已经有了模型计算出的后验概率：

P(Y=肿瘤|X) = 0.3 

P(Y=正常|X) = 0.7

我们可以有以下分析：

如果预测为正常人，那么损失为$0.3*100+0.7*0 = 30$

如果预测为肿瘤，那么损失为$0.3*0+0.7*1 = 0.7$

所以我们选择预测为肿瘤，因为损失更小

如果我们有多个类别，那么我们就需要计算每个类别的期望损失，然后选择最小的那个作为预测结果