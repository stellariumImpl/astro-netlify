---
title: "Probabilistic Approach in Classification"
description: "Probabilistic approaches provide a robust framework for classification and decision-making by modeling uncertainty. Bayesian methods, such as Naïve Bayes and Bayesian inference, leverage prior knowledge and observed data to make predictions. This article explores key probabilistic techniques and their applications in machine learning."
image: "../assets/wallhaven-wepyox.jpg"
createdAt: 03-17-2025
draft: false
tags:
  - 分类问题
  - 贝叶斯方法
  - 朴素贝叶斯
---

## 贝叶斯基础

概率分类的核心是**贝叶斯定理**，一个简单却深刻的公式，用于关联条件概率：

$$
P(h|D) = \frac{P(D|h)P(h)}{P(D)}
$$

其中，$P(h)$ 是假设 $h$ 的先验概率（例如邮件是垃圾邮件的概率），$P(D|h)$ 是给定 $h$ 时观测到数据 $D$ 的似然，$P(D)$ 是数据的边缘概率，而 $P(h|D)$ 是后验概率——在看到 $D$ 后对 $h$ 的更新信念。这个定理为我们提供了一种随着新证据出现而更新信念的结构化方法，非常适合分类任务。

在实践中，贝叶斯方法有两个关键作用：
1. **实用学习算法**：它支持朴素贝叶斯分类器和贝叶斯网络等，通过结合先验知识与训练数据进行预测。
2. **概念框架**：它为评估其他算法提供了“黄金标准”，并为奥卡姆剃刀等原则（在证据相等时偏好简单模型）提供了洞见。

## 生成模型与判别模型

分类有两种主要方式：判别式和生成式。判别模型（如逻辑回归）直接建模后验概率 $P(y|x)$，预测给定特征 $x$ 的类别 $y$。生成模型则建模目标 $y$ 和特征 $x$ 的联合分布 $P(y, x)$，允许我们生成新数据点并通过贝叶斯定理推导 $P(y|x)$：

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

例如，假设我们要根据鱼的长度分类三文鱼或海鲈鱼。生成方法可能分别建模 $P(x|y=\text{三文鱼})$ 和 $P(x|y=\text{海鲈鱼})$，使用先验概率 $P(y)$ 加权每个类别。这与绘制决策边界不同，它关注每个类别在概率上的“模样”。

## 朴素贝叶斯：简单与力量的结合

**朴素贝叶斯分类器** 是生成概率模型的典型代表。它假设给定类别标签时，特征（属性）之间条件独立——尽管这一假设在现实中常被违反，但其效果依然惊人。分类器通过以下公式预测实例 $x = \langle x_1, x_2, \ldots, x_n \rangle$ 的最可能类别 $v_{NB}$：

$$
v_{NB} = \arg \max_{v_j \in V} P(v_j) \prod_i P(x_i | v_j)
$$

### 实践示例：PlayTennis
以 PlayTennis 数据集为例，我们根据天气、温度、湿度、风力等属性预测是否打网球。通过训练数据，我们估计概率，如 $P(\text{晴天}|\text{是})$ 和 $P(\text{有风=true}|\text{否})$。对于新实例（如 $\langle \text{晴天, 凉爽, 高湿度, 有风} \rangle$），我们计算“yes”和“no”的似然，归一化为后验概率，并选择较高的那个。结果可能是“no”，概率为 0.795，展示了朴素贝叶斯的简单而有效的决策过程。

### 优点与缺点
- **优点**：朴素贝叶斯计算快、易实现，在小数据集或类别特征上表现良好，尤其适用于文本分类（如垃圾邮件检测）。
- **缺点**：独立性假设很少完全成立，且零频率问题（训练中未见某特征-类别组合）可能导致预测失败，除非使用平滑技术（如拉普拉斯估计）。此外，它的后验概率估计往往过于自信。

### 应用：文本分类
朴素贝叶斯在文本分类中大放异彩，例如将邮件分为垃圾邮件或正常邮件。使用“词袋”模型，它假设词的出现独立于类别。在 20 Newsgroups 数据集上，它能以 89% 的准确率分类 20 个主题的文章——对于其简单性而言相当惊艳。无论是采用多元伯努利模型（词出现/未出现）还是多项式模型（词计数），朴素贝叶斯都能利用词概率进行稳健预测。

## 超越朴素贝叶斯：贝叶斯推理

虽然朴素贝叶斯实用性强，但更广泛的贝叶斯方法提供了更深的洞察。两种关键推理技术包括：

1. **最大后验概率 (MAP)**：选择最大化 $P(h|D)$ 的假设 $h_{MAP}$，平衡似然与先验：

$$
h_{MAP} = \arg \max_{h \in H} P(D|h)P(h)
$$

例如，在癌症诊断中，即使检测结果为阳性，由于 $P(\text{癌症}) = 0.008$ 较低，MAP 可能倾向于“无癌症”。

2. **最大似然 (ML)**：若先验均匀，MAP 简化为 ML，最大化 $P(D|h)$：

$$
h_{ML} = \arg \max_{h \in H} P(D|h)
$$

### 贝叶斯最优分类器
我们的终极目标往往不是最佳假设，而是对新实例 $x$ 的最佳预测。**贝叶斯最优分类器** 通过对所有假设加权平均实现这一点：

$$
\arg \max_{v_j \in V} \sum_{h_i \in H} P(v_j | h_i) P(h_i | D)
$$

例如，若三个假设对 $x$ 的预测不同（一个为“+”，两个为“−”），最优选择是“−”，尽管 MAP 假设可能是“+”。理论上，没有其他方法能平均优于此，但计算成本高，常需用 Gibbs 分类器（随机抽样假设）近似。

### 处理不确定性：期望损失
贝叶斯方法还通过**期望损失**考虑决策成本。在癌症例子中，将癌症误判为无的代价可能是反向误判的 10 倍。通过最小化 $E[L] = \sum_h \lambda(\alpha|h)P(h|x)$，我们可能预测“癌症”，优先考虑安全性而非单纯概率。

## 概率模型中的归纳偏置

每个模型都有**归纳偏置**——指导从有限数据泛化的假设。在朴素贝叶斯中，这是独立性假设；在线性回归中，是线性假设。贝叶斯方法明确这一点，量化不确定性并允许调整假设（如实值数据的正态噪声假设）。强偏置（如朴素贝叶斯的简单性）降低方差但可能引入偏差，而弱偏置增加灵活性却可能过拟合。

## 为什么选择概率方法？

概率方法的优势在于：
- 明确建模不确定性，不同于确定性方法。
- 通过先验适应新数据，在动态环境中表现稳健。
- 支持多样应用，从垃圾邮件过滤到医疗诊断。
